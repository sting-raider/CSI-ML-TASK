{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "id": "401a055eaea4f2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_preprocess_image(img_path, img_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Load an image, convert to grayscale, resize, apply Gaussian blur,\n",
    "    and normalize pixel values to [0,1].\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def extract_features(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Warning: Unable to load image {image_path}\")\n",
    "        return None  # Skip this image\n",
    "    \n",
    "    image = cv2.resize(image, (128, 128))  # Resize for consistency\n",
    "    \n",
    "    # HOG Feature Extraction\n",
    "    hog_features, _ = hog(image, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=True)\n",
    "    \n",
    "    # LBP Feature Extraction\n",
    "    lbp = local_binary_pattern(image, P=24, R=3, method=\"uniform\")\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(27), density=True)\n",
    "    \n",
    "    return np.hstack((hog_features, lbp_hist))\n",
    "\n",
    "\n",
    "def load_dataset(real_folder, fake_folder):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for file in os.listdir(real_folder):\n",
    "        if file.lower() == \"thumbs.db\":\n",
    "            continue\n",
    "        img_path = os.path.join(real_folder, file)\n",
    "        features = extract_features(img_path)\n",
    "        if features is not None:\n",
    "            X.append(features)\n",
    "            y.append(1)\n",
    "    \n",
    "    for file in os.listdir(fake_folder):\n",
    "        if file.lower() == \"thumbs.db\":\n",
    "            continue\n",
    "        img_path = os.path.join(fake_folder, file)\n",
    "        features = extract_features(img_path)\n",
    "        if features is not None:\n",
    "            X.append(features)\n",
    "            y.append(0)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n"
   ],
   "id": "3dde484c977ab702"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load dataset\n",
    "dataset_dir = './signature_dataset'\n",
    "real_path = os.path.join(dataset_dir, \"full_org\")\n",
    "fake_path = os.path.join(dataset_dir, \"full_forg\")\n",
    "X, y = load_dataset(real_path, fake_path)\n",
    "\n",
    "# Split dataset (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train an SVM classifier (using RBF kernel)\n",
    "clf = SVC(kernel='rbf', probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"=== CONFUSION MATRIX ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save the model for later use\n",
    "joblib.dump(clf, \"svm_signature_classifier.pkl\")"
   ],
   "id": "8065521d9ade0e47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 4: Setup for Neural Style Transfer (NST) and TensorBoard logging\n",
    "\n",
    "\n",
    "# Set device and image size for NST\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "imsize = 256  # Desired image size for NST\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter\n",
    "writer = SummaryWriter(\"runs/nst_signature\")\n",
    "\n",
    "# Define transforms for loading/unloading images (convert to 3-channel RGB for VGG)\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def image_loader(image_path):\n",
    "    \"\"\"Load and preprocess an image for NST.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    \"\"\"Display a tensor as an image.\"\"\"\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n"
   ],
   "id": "84d4437a2bc8b6fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 5: Define NST loss classes (Content and Style Loss) and helper functions\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # Detach target to avoid tracking history\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = nn.functional.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "def gram_matrix(input):\n",
    "    batch_size, feature_maps, h, w = input.size()\n",
    "    features = input.view(batch_size * feature_maps, h * w)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(batch_size * feature_maps * h * w)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = nn.functional.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "# Load pre-trained VGG19 for feature extraction\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "# Normalization parameters for VGG\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    \"\"\"Module to normalize input images for VGG.\"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = mean.clone().detach().view(-1, 1, 1)\n",
    "        self.std = std.clone().detach().view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img, content_img,\n",
    "                               content_layers=['conv_4'],\n",
    "                               style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']):\n",
    "    \"\"\"\n",
    "    Build the model and attach style and content loss layers.\n",
    "    \"\"\"\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # Increment count for conv layers\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f'conv_{i}'\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f'relu_{i}'\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f'pool_{i}'\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f'bn_{i}'\n",
    "        else:\n",
    "            raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n",
    "        \n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # Trim the model after the last loss layer\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses\n"
   ],
   "id": "63962976017c22dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "#UNFINISHED LOTS OF PROBLEMS STILL FIGURING IT OUT!!\n",
    "\n",
    "\n",
    "def load_artbench_bin(bin_file):\n",
    "    \"\"\"Load ArtBench-10 binary file.\"\"\"\n",
    "    with open(bin_file, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "    \n",
    "    # Each image in ArtBench-10 is 32x32x3 (3072 bytes per image) + 1 byte for label\n",
    "    img_size = 32 * 32 * 3  # Assuming 32x32 version\n",
    "    record_size = img_size + 1  # 1 byte for label\n",
    "    \n",
    "    num_images = len(raw_data) // record_size  # Compute number of images\n",
    "    images = np.zeros((num_images, 3, 32, 32), dtype=np.uint8)\n",
    "    labels = np.zeros(num_images, dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Extract label (first byte of each record)\n",
    "        offset = i * record_size\n",
    "        labels[i] = raw_data[offset]\n",
    "        \n",
    "        # Extract image data (remaining bytes)\n",
    "        img_start = offset + 1  # Skip label byte\n",
    "        img_data = np.frombuffer(raw_data[img_start:img_start + img_size], dtype=np.uint8)\n",
    "        images[i] = img_data.reshape(3, 32, 32)  # Reshape into (3, 32, 32)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps=300,\n",
    "                       style_weight=1e6, content_weight=1):\n",
    "    \"\"\"\n",
    "    Run NST optimization to generate a synthetic signature.\n",
    "    \"\"\"\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter('./runs/style_transfer')\n",
    "    \n",
    "    # Check that all inputs are on the same device\n",
    "    device = content_img.device\n",
    "    \n",
    "    # Make sure model and normalization values are on the same device\n",
    "    normalization_mean = normalization_mean.to(device)\n",
    "    normalization_std = normalization_std.to(device)\n",
    "    \n",
    "    # Get the style model and losses\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(\n",
    "        cnn, normalization_mean, normalization_std, style_img, content_img)\n",
    "    \n",
    "    # Optimize only the image\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    \n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "        def closure():\n",
    "            # Correct the values of updated input image\n",
    "            with torch.no_grad():\n",
    "                input_img.clamp_(0, 1)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            \n",
    "            style_score = sum(sl.loss for sl in style_losses)\n",
    "            content_score = sum(cl.loss for cl in content_losses)\n",
    "            \n",
    "            loss = style_weight * style_score + content_weight * content_score\n",
    "            loss.backward()\n",
    "            \n",
    "            # Log metrics to TensorBoard\n",
    "            writer.add_scalar('Loss/Style', style_score.item(), run[0])\n",
    "            writer.add_scalar('Loss/Content', content_score.item(), run[0])\n",
    "            writer.add_scalar('Loss/Total', loss.item(), run[0])\n",
    "            \n",
    "            run[0] += 1\n",
    "            return loss\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "    \n",
    "    # Final correction\n",
    "    with torch.no_grad():\n",
    "        input_img.clamp_(0, 1)\n",
    "        \n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "    \n",
    "    return input_img\n",
    "content_dir = './signature_dataset/full_org/'\n",
    "style_dir = './artbench10/'\n",
    "save_dir = './synthetic_forged/'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "style_bin_files = [os.path.join(style_dir, f) for f in os.listdir(style_dir) if f.startswith('data_batch_') and f.endswith('.bin')]\n",
    "\n",
    "# Load all ArtBench images into memory\n",
    "style_images = np.concatenate([load_artbench_bin(f) for f in style_bin_files], axis=0)\n",
    "\n",
    "for content_file in os.listdir(content_dir):\n",
    "    if not content_file.endswith(('jpg', 'png')):\n",
    "        continue\n",
    "    \n",
    "    content_img_path = os.path.join(content_dir, content_file)\n",
    "    style_img_data = random.choice(style_images)  # Randomly select a style image\n",
    "    \n",
    "    style_img = torch.tensor(style_img_data / 255.0).float()\n",
    "    transform = transforms.ToPILImage()\n",
    "    style_img = transform(style_img)\n",
    "    \n",
    "    content_img = image_loader(content_img_path)\n",
    "    input_img = content_img.clone()\n",
    "    \n",
    "    output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                                content_img, style_img, input_img, num_steps=300)\n",
    "    \n",
    "    output_image = output.cpu().clone().squeeze(0)\n",
    "    output_image = unloader(output_image)\n",
    "    output_image.save(os.path.join(save_dir, content_file))\n",
    "    \n",
    "    print(f\"Saved: {content_file} -> {save_dir}\")\n"
   ],
   "id": "4ec904aa683fd8a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 7: Evaluate the SVM classifier on the generated synthetic signature\n",
    "\n",
    "# Read the synthetic image (as grayscale) for feature extraction\n",
    "synthetic_img = cv2.imread(\"synthetic_signature.png\", cv2.IMREAD_GRAYSCALE)\n",
    "synthetic_img = cv2.resize(synthetic_img, (256, 256))\n",
    "synthetic_img = synthetic_img / 255.0\n",
    "\n",
    "# Extract features and reshape for the classifier\n",
    "synthetic_features = extract_features(synthetic_img).reshape(1, -1)\n",
    "\n",
    "# Predict using the pre-trained SVM classifier\n",
    "prediction = clf.predict(synthetic_features)\n",
    "probabilities = clf.predict_proba(synthetic_features)\n",
    "\n",
    "print(\"=== CLASSIFIER PREDICTION ON SYNTHETIC SIGNATURE ===\")\n",
    "print(\"Prediction (0: real, 1: forged):\", prediction[0])\n",
    "print(\"Probabilities:\", probabilities[0])\n"
   ],
   "id": "5baa8497c3f4fba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9a969016aa59e527"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
